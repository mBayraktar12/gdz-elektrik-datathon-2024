{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29376dea-7f61-43f2-b114-5dc6611e2b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from prophet import Prophet\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "\n",
    "from catboost import CatBoostRegressor, Pool\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import optuna\n",
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMRegressor\n",
    "import warnings\n",
    "\n",
    "\n",
    "pd.options.display.max_rows = 250\n",
    "pd.options.display.max_columns = 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b512d82-30f6-47e3-9a8b-1ce0ebf40662",
   "metadata": {},
   "outputs": [],
   "source": [
    "holidays = pd.read_csv('/kaggle/input/gdz-elektrik-datathon-2024/holidays.csv')\n",
    "sub = pd.read_csv('/kaggle/input/gdz-elektrik-datathon-2024/sample_submission.csv')\n",
    "test = pd.read_csv('/kaggle/input/gdz-elektrik-datathon-2024/test.csv')\n",
    "train = pd.read_csv('/kaggle/input/gdz-elektrik-datathon-2024/train.csv')\n",
    "weather = pd.read_csv('/kaggle/input/gdz-elektrik-datathon-2024/weather.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57d47b9-9d67-40f3-9856-66b143090917",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98bf78e-cc86-40ef-a6fc-061b0207d825",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_shift = ['bildirimli_sum']\n",
    "#,'t_2m-C_mean''effective_cloud_cover-p_mean', 'global_rad-W_mean', 'relative_humidity_2m-p_mean', 'wind_dir_10m-d_mean', 'wind_speed_10m-ms_mean', 'prob_precip_1h-p_mean', 't_apparent-C_mean']\n",
    "\n",
    "\n",
    "cols_to_roll = ['bildirimli_sum', 't_2m-C_mean', 'relative_humidity_2m-p_mean', 'prob_precip_1h-p_mean', 't_apparent-C_mean']\n",
    "\n",
    "cols_to_cluster = ['bildirimli_sum']\n",
    "\n",
    "lags = np.arange(0,51,5)[1:]\n",
    "\n",
    "rolls = np.arange(0,48,6)[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9906c1-968d-4cb1-9a82-0db64534af24",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([train, test], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b6e007-52cf-47d8-92b6-69841e36a2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "holidays.rename(columns={'Yıl': 'year', 'Ay': 'month', 'Gün': 'day'}, inplace=True)\n",
    "\n",
    "holidays['tarih'] = pd.to_datetime(holidays[['year', 'month', 'day']])\n",
    "\n",
    "holidays = holidays.drop(['year', 'month', 'day'], axis=1)\n",
    "\n",
    "holidays.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67305c11-3735-4144-98ac-bb294f1e212f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['tarih'] = pd.to_datetime(data['tarih'])\n",
    "\n",
    "merged_df = pd.merge(data, holidays, on='tarih', how='left')\n",
    "\n",
    "merged_df['Tatil Adı'] = merged_df['Tatil Adı'].fillna('Not a Holiday')\n",
    "\n",
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b14e54-73ea-4f7f-8d9b-d77cdab9bd37",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['tarih'] = pd.to_datetime(data['tarih'])\n",
    "weather['date'] = pd.to_datetime(weather['date'])\n",
    "\n",
    "# Extract date and hour\n",
    "weather['date_only'] = weather['date'].dt.date\n",
    "weather['hour_only'] = weather['date'].dt.hour\n",
    "weather = weather.drop(['date'], axis=1)\n",
    "weather['name'] = weather['name'].str.lower()\n",
    "weather.rename(columns={'date_only': 'date', 'name':'ilce'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfcccccb-7432-4f80-8e46-34c0e95ae33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate weather data by date and district to daily averages\n",
    "daily_weather = weather.groupby(['date', 'ilce']).agg({\n",
    "    't_2m:C': ['mean', 'std', 'min', 'max', 'sum'],\n",
    "    'effective_cloud_cover:p': ['mean', 'std', 'min', 'max', 'sum'],\n",
    "    'global_rad:W': ['mean', 'std', 'min', 'max', 'sum'],\n",
    "    'relative_humidity_2m:p': ['mean', 'std', 'min', 'max', 'sum'],\n",
    "    'wind_dir_10m:d': ['mean', 'std', 'min', 'max', 'sum'],\n",
    "    'wind_speed_10m:ms': ['mean', 'std', 'min', 'max', 'sum'],\n",
    "    'prob_precip_1h:p': ['mean', 'std', 'min', 'max', 'sum'],\n",
    "    't_apparent:C': ['mean', 'std', 'min', 'max', 'sum'],\n",
    "}).reset_index()\n",
    "\n",
    "\n",
    "\n",
    "daily_weather.columns = ['_'.join(col).strip() for col in daily_weather.columns.values]\n",
    "\n",
    "# Convert 'date' back to datetime to match with merged_df's 'tarih'\n",
    "daily_weather['date_'] = pd.to_datetime(daily_weather['date_'])\n",
    "\n",
    "# Merge with the previous merged data (merged_df)\n",
    "final_merged_df = pd.merge(merged_df, daily_weather, left_on=['tarih', 'ilce'], right_on=['date_', 'ilce_'], how='left')\n",
    "\n",
    "# Drop redundant 'date' column\n",
    "final_merged_df = final_merged_df.drop('date_', axis=1)\n",
    "final_merged_df = final_merged_df.drop('ilce_', axis=1)\n",
    "# \n",
    "# Display the first few rows of the final merged data\n",
    "final_merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044ea7ba-9102-48d6-8c42-32733dfb41ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_features(df, time_col='tarih'):\n",
    "\n",
    "    df_ = df.copy()\n",
    "\n",
    "#     df_[\"hour\"] = df_[time_col].dt.hour\n",
    "#     df_['day'] = df_[time_col].dt.day\n",
    "    df_[\"year\"] = df_[time_col].dt.year\n",
    "    df_[\"month\"] = df_[time_col].dt.month\n",
    "    df_[\"quarter\"] = df_[time_col].dt.quarter\n",
    "    df_[\"dayofweek\"] = df_[time_col].dt.dayofweek\n",
    "    df_[\"dayofyear\"] = df_[time_col].dt.dayofyear\n",
    "    df_[\"dayofmonth\"] = df_[time_col].dt.day\n",
    "    df_[\"weekofyear\"] = df_[time_col].dt.isocalendar().week\n",
    "    df_[\"season\"] = df_[\"month\"] % 12 // 3 + 1\n",
    "\n",
    "    # Categorical Features\n",
    "    df_[\"is_weekend\"] = df_[time_col].apply(lambda x: x.weekday() > 4).astype(int)\n",
    "#     df_[\"is_night\"] = df_[\"hour\"].apply(lambda x: x in [0, 1, 2, 3, 4, 5]).astype(int)\n",
    "    df_['is_month_start'] = df_[time_col].dt.is_month_start.astype(int)\n",
    "    df_['is_month_end'] = df_[time_col].dt.is_month_end.astype(int)\n",
    "    df_['is_quarter_start'] = df_[time_col].dt.is_quarter_start.astype(int)\n",
    "    df_['is_quarter_end'] = df_[time_col].dt.is_quarter_end.astype(int)\n",
    "    df_['is_year_start'] = df_[time_col].dt.is_year_start.astype(int)\n",
    "    df_['is_year_end'] = df_[time_col].dt.is_year_end.astype(int)\n",
    "\n",
    "    # Cyclic Features\n",
    "    df_[\"dayofyear_sin\"] = np.sin(2 * np.pi * df_[\"dayofyear\"] / df_[\"dayofyear\"].max())\n",
    "    df_[\"dayofyear_cos\"] = np.cos(2 * np.pi * df_[\"dayofyear\"] / df_[\"dayofyear\"].max())\n",
    "    df_[\"month_sin\"] = np.sin(2 * np.pi * df_[\"month\"] / df_[\"month\"].max())\n",
    "    df_[\"month_cos\"] = np.cos(2 * np.pi * df_[\"month\"] / df_[\"month\"].max())\n",
    "\n",
    "#     df_[\"weekofyear_sin\"] = np.sin(2 * np.pi * df_[\"weekofyear\"] / df_[\"weekofyear\"].max()) ####\n",
    "#     df_[\"weekofyear_cos\"] = np.cos(2 * np.pi * df_[\"weekofyear\"] / df_[\"weekofyear\"].max())#######\n",
    "\n",
    "#     df_[\"dayofweek_sin\"] = np.sin(2 * np.pi * df_[\"dayofweek\"] / 7)###########\n",
    "#     df_[\"dayofweek_cos\"] = np.cos(2 * np.pi * df_[\"dayofweek\"] / 7)############\n",
    "\n",
    "#     df_[\"quarter_sin\"] = np.sin(2 * np.pi * df_[\"quarter\"] / 4)#################\n",
    "#     df_[\"quarter_cos\"] = np.cos(2 * np.pi * df_[\"quarter\"] / 4)################\n",
    "\n",
    "    df_['year_season'] = df_['year'].astype(str) + \"_\" + df_['season'].astype(str)\n",
    "\n",
    "    return df_\n",
    "\n",
    "def lag_features(\n",
    "    dataframe: pd.DataFrame, cols: list, lags: list, fillna: bool = False\n",
    ") -> pd.DataFrame:\n",
    "    df_ = dataframe.copy()\n",
    "    for col in cols:\n",
    "        for lag in lags:\n",
    "            df_[f\"{col}_{lag}_lag\"] = df_[col].shift(lag)\n",
    "            if fillna:\n",
    "                df_[f\"{col}_{lag}_lag\"] = df_[f\"{col}_{lag}_lag\"].fillna(method=\"bfill\")\n",
    "                df_[f\"{col}_{lag}_lag\"] = df_[f\"{col}_{lag}_lag\"].fillna(method=\"ffill\")\n",
    "    return df_\n",
    "\n",
    "\n",
    "def roll_features(\n",
    "    dataframe: pd.DataFrame, cols: list, windows: list, fillna: bool = False\n",
    "):\n",
    "    df_ = dataframe.copy()\n",
    "    for col in cols:\n",
    "        for w in windows:\n",
    "            df_[f\"rolling_{w}_mean_{col}\"] = df_[col].rolling(w, min_periods=1).mean()\n",
    "            df_[f\"rolling_{w}_std_{col}\"] = df_[col].rolling(w, min_periods=1).std()\n",
    "            df_[f\"rolling_{w}_min_{col}\"] = df_[col].rolling(w, min_periods=1).min()\n",
    "            df_[f\"rolling_{w}_max_{col}\"] = df_[col].rolling(w, min_periods=1).max()\n",
    "            if fillna:\n",
    "                df_[f\"rolling_{w}_mean_{col}\"] = df_[f\"rolling_{w}_mean_{col}\"].fillna(\n",
    "                    method=\"bfill\"\n",
    "                )\n",
    "                df_[f\"rolling_{w}_std_{col}\"] = df_[f\"rolling_{w}_std_{col}\"].fillna(\n",
    "                    method=\"bfill\"\n",
    "                )\n",
    "                df_[f\"rolling_{w}_min_{col}\"] = df_[f\"rolling_{w}_min_{col}\"].fillna(\n",
    "                    method=\"bfill\"\n",
    "                )\n",
    "                df_[f\"rolling_{w}_max_{col}\"] = df_[f\"rolling_{w}_max_{col}\"].fillna(\n",
    "                    method=\"bfill\"\n",
    "                )\n",
    "                df_[f\"rolling_{w}_mean_{col}\"] = df_[f\"rolling_{w}_mean_{col}\"].fillna(\n",
    "                    method=\"ffill\"\n",
    "                )\n",
    "                df_[f\"rolling_{w}_std_{col}\"] = df_[f\"rolling_{w}_std_{col}\"].fillna(\n",
    "                    method=\"ffill\"\n",
    "                )\n",
    "                df_[f\"rolling_{w}_min_{col}\"] = df_[f\"rolling_{w}_min_{col}\"].fillna(\n",
    "                    method=\"ffill\"\n",
    "                )\n",
    "                df_[f\"rolling_{w}_max_{col}\"] = df_[f\"rolling_{w}_max_{col}\"].fillna(\n",
    "                    method=\"ffill\"\n",
    "                )\n",
    "\n",
    "    return df_\n",
    "\n",
    "\n",
    "def ewma(dataframe: pd.DataFrame, cols: list, lags: list, fillna: bool = True) -> pd.DataFrame:\n",
    "    df_ = dataframe.copy()\n",
    "    for col in cols:\n",
    "        for lag in lags:\n",
    "            df_[f\"{col}_{lag}_lag_ewm\"] = df_[col].shift(lag).ewm(alpha=0.95).mean()\n",
    "            if fillna:\n",
    "                df_[f\"{col}_{lag}_lag_ewm\"] = df_[f\"{col}_{lag}_lag\"].fillna(method=\"bfill\")\n",
    "                df_[f\"{col}_{lag}_lag_ewm\"] = df_[f\"{col}_{lag}_lag\"].fillna(method=\"ffill\")\n",
    "    return df_\n",
    "\n",
    "def feature_engineering(dataframe: pd.DataFrame) -> pd.DataFrame:\n",
    "\n",
    "    df_ = dataframe.copy()\n",
    "\n",
    "    ilce_grouped_yearly = df_.groupby(['ilce','year'])\n",
    "    df_['bildirimli_sum_mean_yearly'] = ilce_grouped_yearly['bildirimli_sum'].transform('mean')\n",
    "    df_['bildirimli_sum_std_yearly'] = ilce_grouped_yearly['bildirimli_sum'].transform('std')\n",
    "    df_['bildirimli_sum_var_yearly'] = ilce_grouped_yearly['bildirimli_sum'].transform('var')\n",
    "    df_['bildirimli_sum_median_yearly'] = ilce_grouped_yearly['bildirimli_sum'].transform('median')\n",
    "    df_['bildirimli_sum_min_yearly'] = ilce_grouped_yearly['bildirimli_sum'].transform('min')\n",
    "    df_['bildirimli_sum_max_yearly'] = ilce_grouped_yearly['bildirimli_sum'].transform('max')\n",
    "    df_['bildirimli_sum_skew_yearly'] = ilce_grouped_yearly['bildirimli_sum'].transform('skew')\n",
    "    df_['bildirimli_sum_25th_yearly'] = ilce_grouped_yearly['bildirimli_sum'].transform(lambda x: x.quantile(0.25))\n",
    "    df_['bildirimli_sum_75th_yearly'] = ilce_grouped_yearly['bildirimli_sum'].transform(lambda x: x.quantile(0.75))\n",
    "    df_['bildirimli_sum_range'] = ilce_grouped_yearly['bildirimli_sum'].transform(lambda x: x.max() - x.min())\n",
    "#     df_['bildirimli_sum_iqr'] = ilce_grouped_yearly['bildirimli_sum'].transform(lambda x: x.quantile(0.75) - x.quantile(0.25))\n",
    "    \n",
    "    df_.columns = df_.columns.str.replace(':', '-')\n",
    "\n",
    "    df_['high_wind_flag'] = (df_['wind_speed_10m-ms_mean'] > 6).astype(int)\n",
    "\n",
    "    df_['extreme_high_temp_flag'] = (df_['t_2m-C_mean'] > 35).astype(int) ################\n",
    "    df_['extreme_low_temp_flag'] = (df_['t_2m-C_mean'] < 0).astype(int)\n",
    "\n",
    "    df_['temp_humid'] = df_['t_2m-C_mean'] * df_['relative_humidity_2m-p_mean']\n",
    "#     df_['temp_apparent_humid'] = df_['t_apparent-C_mean'] * df_['relative_humidity_2m-p_mean']\n",
    "    df_['wind_precip'] = df_['wind_speed_10m-ms_mean'] * df_['prob_precip_1h-p_mean']\n",
    "    df_['cloud_precip'] = df_['effective_cloud_cover-p_mean']* df_['prob_precip_1h-p_mean']\n",
    "\n",
    "    df_['wind_dir_sin'] = np.sin(1 * 2 * np.pi * df_['wind_dir_10m-d_mean'] / 360)\n",
    "    df_['wind_dir_cos'] = np.cos(1 * 2 * np.pi * df_['wind_dir_10m-d_mean'] / 360)\n",
    "\n",
    "    df_[\"WindSpeed_sin_component\"] = df_[\"wind_speed_10m-ms_mean\"] * df_[\"wind_dir_sin\"]\n",
    "    df_[\"WindSpeed_cos_component\"] = df_[\"wind_speed_10m-ms_mean\"] * df_[\"wind_dir_cos\"]\n",
    "\n",
    "    # df_[['city', 'district']] = df_['ilce'].str.split('-', expand=True)\n",
    "\n",
    "    return df_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e15e0b-9370-4bab-8b60-fe4a514cd881",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_dtypes(df):\n",
    "\n",
    "    df_ = df.copy()\n",
    "\n",
    "    for col in df_.select_dtypes(include=['object']).columns:\n",
    "        df_[col] = df_[col].astype('category')\n",
    "\n",
    "#     for col in df_.select_dtypes(include=['float64']).columns:\n",
    "#         df_[col] = df_[col].astype('float32')\n",
    "\n",
    "#     for col in df_.select_dtypes(include=['int64']).columns:\n",
    "#         df_[col] = df_[col].astype('int16')\n",
    "\n",
    "    return df_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93fa287f-feb4-4430-901a-ad09bec52899",
   "metadata": {},
   "outputs": [],
   "source": [
    "inal_merged_df = time_features(final_merged_df)\n",
    "\n",
    "final_merged_df = feature_engineering(final_merged_df)\n",
    "\n",
    "final_merged_df = lag_features(final_merged_df, cols_to_shift, lags)\n",
    "\n",
    "# final_merged_df = ewma(final_merged_df, cols_to_roll, lags)\n",
    "\n",
    "# final_merged_df = roll_features(final_merged_df, cols_to_roll, rolls)\n",
    "\n",
    "# final_merged_df = expand_features(final_merged_df, cols_to_shift)\n",
    "\n",
    "final_merged_df = convert_dtypes(final_merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2005c1-46f3-42ed-9afb-be1a2f9bcc49",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols = final_merged_df.select_dtypes(include=['category']).columns\n",
    "categorical_cols = categorical_cols.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8002f2a3-506c-4552-b54e-4b1a65699303",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = final_merged_df.loc[(final_merged_df['tarih'] < '2024-02-01')]\n",
    "test = final_merged_df.loc[(final_merged_df['tarih'] > '2024-01-31')]\n",
    "\n",
    "val = train.loc[(train['tarih'] > '2024-01-04')]\n",
    "train = train.loc[(train['tarih'] < '2024-01-05')]\n",
    "train.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aee2313-d52c-4922-a157-da66944e346f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cb_params_42 = {\n",
    "             'iterations': 10000,\n",
    "             'eval_metric': 'MAPE',\n",
    "            'loss_function': 'MAPE',\n",
    "            'early_stopping_rounds': 1000,\n",
    "            'use_best_model': True,\n",
    "            'random_state': 42,\n",
    "            'task_type': \"CPU\",\n",
    "            'verbose': 1000,\n",
    "            'depth': 5\n",
    "#             'learning_rate': 0.29017630492476376, \n",
    "#              'depth': 4, \n",
    "#              'l2_leaf_reg': 7.2678866855697075,\n",
    "#              'bagging_temperature': 0.6339522226899619,\n",
    "#              'border_count': 108, \n",
    "#              'min_data_in_leaf': 68,\n",
    "          }\n",
    "\n",
    "cb_params_0 = {\n",
    "             'iterations': 10000,\n",
    "             'eval_metric': 'MAPE',\n",
    "            'loss_function': 'MAPE',\n",
    "            'early_stopping_rounds': 1000,\n",
    "            'use_best_model': True,\n",
    "            'random_state': 0,\n",
    "            'task_type': \"CPU\",\n",
    "            'verbose': 1000,\n",
    "            'depth': 4\n",
    "#             'learning_rate': 0.29017630492476376, \n",
    "#              'depth': 4, \n",
    "#              'l2_leaf_reg': 7.2678866855697075,\n",
    "#              'bagging_temperature': 0.6339522226899619,\n",
    "#              'border_count': 108, \n",
    "#              'min_data_in_leaf': 68,\n",
    "          }\n",
    "\n",
    "cb_params_1 = {\n",
    "             'iterations': 10000,\n",
    "             'eval_metric': 'MAPE',\n",
    "            'loss_function': 'MAPE',\n",
    "            'early_stopping_rounds': 1000,\n",
    "            'use_best_model': True,\n",
    "            'random_state': 1,\n",
    "            'task_type': \"CPU\",\n",
    "            'verbose': 1000,\n",
    "            'depth': 8\n",
    "#             'learning_rate': 0.29017630492476376, \n",
    "#              'depth': 4, \n",
    "#              'l2_leaf_reg': 7.2678866855697075,\n",
    "#              'bagging_temperature': 0.6339522226899619,\n",
    "#              'border_count': 108, \n",
    "#              'min_data_in_leaf': 68,\n",
    "          }\n",
    "\n",
    "cb_params_986 = {\n",
    "             'iterations': 10000,\n",
    "             'eval_metric': 'MAPE',\n",
    "            'loss_function': 'MAPE',\n",
    "            'early_stopping_rounds': 1000,\n",
    "            'use_best_model': True,\n",
    "            'random_state': 986,\n",
    "            'task_type': \"CPU\",\n",
    "            'verbose': 1000,\n",
    "            'depth': 7\n",
    "#             'learning_rate': 0.29017630492476376, \n",
    "#              'depth': 4, \n",
    "#              'l2_leaf_reg': 7.2678866855697075,\n",
    "#              'bagging_temperature': 0.6339522226899619,\n",
    "#              'border_count': 108, \n",
    "#              'min_data_in_leaf': 68,\n",
    "          }\n",
    "\n",
    "cb_params_1073 = {\n",
    "             'iterations': 10000,\n",
    "             'eval_metric': 'MAPE',\n",
    "            'loss_function': 'MAPE',\n",
    "            'early_stopping_rounds': 1000,\n",
    "            'use_best_model': True,\n",
    "            'random_state': 1073,\n",
    "            'task_type': \"CPU\",\n",
    "            'verbose': 1000\n",
    "#             'learning_rate': 0.29017630492476376, \n",
    "#              'depth': 4, \n",
    "#              'l2_leaf_reg': 7.2678866855697075,\n",
    "#              'bagging_temperature': 0.6339522226899619,\n",
    "#              'border_count': 108, \n",
    "#              'min_data_in_leaf': 68,\n",
    "          }\n",
    "\n",
    "cb_params_527 = {\n",
    "             'iterations': 10000,\n",
    "             'eval_metric': 'MAPE',\n",
    "            'loss_function': 'MAPE',\n",
    "            'early_stopping_rounds': 1000,\n",
    "            'use_best_model': True,\n",
    "            'random_state': 527,\n",
    "            'task_type': \"CPU\",\n",
    "            'verbose': 1000\n",
    "#             'learning_rate': 0.29017630492476376, \n",
    "#              'depth': 4, \n",
    "#              'l2_leaf_reg': 7.2678866855697075,\n",
    "#              'bagging_temperature': 0.6339522226899619,\n",
    "#              'border_count': 108, \n",
    "#              'min_data_in_leaf': 68,\n",
    "          }\n",
    "\n",
    "\n",
    "cb_params_333 = {\n",
    "             'iterations': 10000,\n",
    "             'eval_metric': 'MAPE',\n",
    "            'loss_function': 'MAPE',\n",
    "            'early_stopping_rounds': 1000,\n",
    "            'use_best_model': True,\n",
    "            'random_state': 333,\n",
    "            'task_type': \"CPU\",\n",
    "            'verbose': 1000\n",
    "#             'learning_rate': 0.29017630492476376, \n",
    "#              'depth': 4, \n",
    "#              'l2_leaf_reg': 7.2678866855697075,\n",
    "#              'bagging_temperature': 0.6339522226899619,\n",
    "#              'border_count': 108, \n",
    "#              'min_data_in_leaf': 68,\n",
    "          }\n",
    "\n",
    "\n",
    "\n",
    "cb_params_2 = {\n",
    "             'iterations': 10000,\n",
    "             'eval_metric': 'MAPE',\n",
    "            'loss_function': 'MAPE',\n",
    "            'early_stopping_rounds': 1000,\n",
    "            'use_best_model': True,\n",
    "            'random_state': 2,\n",
    "            'task_type': \"CPU\",\n",
    "            'verbose': 1000\n",
    "#             'learning_rate': 0.29017630492476376, \n",
    "#              'depth': 4, \n",
    "#              'l2_leaf_reg': 7.2678866855697075,\n",
    "#              'bagging_temperature': 0.6339522226899619,\n",
    "#              'border_count': 108, \n",
    "#              'min_data_in_leaf': 68,\n",
    "          }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab89d6d-5ddd-4866-9ad7-725a8f1ed106",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_cols = train.select_dtypes(include=['number']).columns.tolist()\n",
    "\n",
    "# Calculate the correlation matrix for all numerical columns\n",
    "full_correlation_matrix = train[numerical_cols].corr()\n",
    "\n",
    "full_correlation_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a495a1eb-9ca7-448b-886a-6a284c6a3dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "param_sets = {\n",
    "    'cb0': cb_params_0,\n",
    "    'cb1': cb_params_1,\n",
    "    'cb42': cb_params_42,\n",
    "    'cb986': cb_params_986,\n",
    "    'cb1073': cb_params_1073,\n",
    "    'cb527': cb_params_527,\n",
    "    'cb333': cb_params_333,\n",
    "    'cb2': cb_params_2,\n",
    "}\n",
    "\n",
    "all_test_results = {}\n",
    "all_preds = []\n",
    "\n",
    "for param_name, params in param_sets.items():\n",
    "    test_all_dates = pd.DataFrame()\n",
    "    all_test_dates = []\n",
    "    val_scores = []\n",
    "    district_list = []\n",
    "\n",
    "    for ilce in train['ilce'].unique():\n",
    "        df_train = train[train['ilce'] == ilce]\n",
    "        df_val = val[val['ilce'] == ilce]\n",
    "        df_test = test[test['ilce'] == ilce]\n",
    "\n",
    "        test_dates = df_test[['tarih']]\n",
    "        print(f'Training with parameters: {param_name}')\n",
    "        print('train:', df_train.shape)\n",
    "        print('val:', df_val.shape)\n",
    "        print('test:', df_test.shape)\n",
    "        print('ilce:', ilce)\n",
    "\n",
    "        X_train = df_train.drop(columns=['bildirimsiz_sum'], axis=1)\n",
    "        X_val = df_val.drop(columns=['bildirimsiz_sum'], axis=1)\n",
    "        X_test = df_test.drop(columns=['bildirimsiz_sum'], axis=1)\n",
    "\n",
    "        y_train = df_train['bildirimsiz_sum']\n",
    "        y_val = df_val['bildirimsiz_sum']\n",
    "\n",
    "        cat_features = [X_train.columns.get_loc(col) for col in categorical_cols]\n",
    "\n",
    "        train_pool = Pool(X_train, y_train, cat_features=cat_features)\n",
    "        validate_pool = Pool(X_val, y_val, cat_features=cat_features)\n",
    "\n",
    "        model = CatBoostRegressor(**params)\n",
    "        model.fit(train_pool, eval_set=validate_pool, use_best_model=True)\n",
    "\n",
    "        val_pred = model.predict(X_val)\n",
    "        test_pred = model.predict(X_test)\n",
    "\n",
    "        test_dates['y_pred'] = test_pred\n",
    "\n",
    "        val_mape = mean_absolute_percentage_error(y_val + 1, val_pred + 1)\n",
    "\n",
    "        print(f'VALIDATION SCORE: {val_mape}')\n",
    "        print(test_dates.head())\n",
    "\n",
    "        all_test_dates.append(test_dates)\n",
    "        val_scores.append(val_mape)\n",
    "\n",
    "        print(\"-\" * 80)\n",
    "\n",
    "    test_all_dates = pd.concat(all_test_dates)\n",
    "    test_all_dates = test_all_dates.sort_values(\"tarih\")\n",
    "    test_all_dates.sort_index(inplace=True)\n",
    "    preds = test_all_dates[\"y_pred\"].values\n",
    "\n",
    "    all_test_results[param_name] = {\n",
    "        'test_dates': test_all_dates,\n",
    "        'val_scores': val_scores,\n",
    "        'avg_val_score': np.mean(val_scores),\n",
    "        'avg_forecasts': np.mean(preds),\n",
    "        'district_list': district_list\n",
    "    }\n",
    "\n",
    "    all_preds.append(preds)\n",
    "\n",
    "    print(f\"CATBOOST Forecasts AVG: {np.mean(preds)}\")\n",
    "    print(f\"CATBOOST VALIDATION SCORE AVG:{np.mean(val_scores)}\")\n",
    "    print(f\"CATBOOST TAHMIN EDILEMEYEN ILCE LISTESI:{district_list}\")\n",
    "\n",
    "# Calculate the final predictions as the weighted average of all individual model predictions\n",
    "weights = 0.125\n",
    "final_preds = np.sum([weights * pred for pred in all_preds], axis=0)\n",
    "\n",
    "print(f\"Final Predictions AVG: {np.mean(final_preds)}\")\n",
    "\n",
    "# Display the results for each parameter set\n",
    "for param_name, results in all_test_results.items():\n",
    "    print(f\"Results for {param_name}:\")\n",
    "    print(f\"Average Validation Score: {results['avg_val_score']}\")\n",
    "    print(f\"Average Forecasts: {results['avg_forecasts']}\")\n",
    "    print(f\"Test Dates Head:\\n{results['test_dates'].head()}\")\n",
    "    print(\"-\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac17b8c-4fb4-4dd6-8210-a02e51128859",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub['bildirimsiz_sum'] = final_preds\n",
    "sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb835bf-3302-4455-ac65-4d215e32c5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.to_csv('submission.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6f268e-5f8f-4e1e-971b-c72d43e192ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
